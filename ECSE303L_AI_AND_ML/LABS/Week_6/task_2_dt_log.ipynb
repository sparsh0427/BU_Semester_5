{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Employee Attrition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "The dataset is available at <strong>\"data/attrition.csv\"</strong> in the respective challenge's repo.<br>\n",
    "This is a fictional data set created by IBM data scientists.\n",
    "\n",
    "#### Features (X)\n",
    "1. Age - Employee's current age. (Numeric)\n",
    "2. BusinessTravel - Frequency of travelling for business (Categorical)\n",
    "    - Travel Frequently\n",
    "    - Travel Rarely\n",
    "    - Not Travel at all\n",
    "3. DailyRate - Daily rate of earning. (Numeric)\n",
    "4. Department - Job-specific Department. (Categorical)\n",
    "    - R&D\n",
    "    - Sales\n",
    "    - HR\n",
    "5. DistanceFromHome (Numeric)\n",
    "6. Education (Numeric) -\n",
    "    - Below College (1)\n",
    "    - College (2)\n",
    "    - Bachelor (3)\n",
    "    - Master (4)\n",
    "    - Doctor (5)\n",
    "7. Education Field (Categorical) -\n",
    "    - Life Sciences\n",
    "    - Medical\n",
    "    - Marketing\n",
    "    - Techincal Degree\n",
    "    - Human Resources\n",
    "    - Other\n",
    "8. EmployeeCount (Numeric)\n",
    "9. EmployeeNumber (Numeric)\n",
    "10. EnvironmentSatisfaction (1-4) (Numeric)\n",
    "11. Gender (Binary)\n",
    "12. HourlyRate (Numeric)\n",
    "13. JobInvolvement (1-4) (Numeric)\n",
    "14. JobLevel (1-5) (Numeric)\n",
    "15. JobRole (Categorical)\n",
    "    - Research Scientist\n",
    "    - Laboratory Technician\n",
    "    - Manufacturing Director\n",
    "    - Healthcare Representative\n",
    "    - Manager\n",
    "    - Research Director\n",
    "    - Sales Executive\n",
    "    - Sales Representative\n",
    "    - Human Resources\n",
    "16. JobSatisfaction (1-4) (Numeric)\n",
    "17. MaritalStatus (Categorical)\n",
    "    - Married\n",
    "    - Single\n",
    "    - Divorced\n",
    "18. MonthlyIncome (Numeric)\n",
    "19. MonthlyRate (Numeric)\n",
    "20. NumCompaniesWorked (Numeric)\n",
    "21. OverTime (Yes/No) (Categorical)\n",
    "22. PercentSalaryHike (Numeric)\n",
    "23. PerformanceRating (1-4) (Numeric)\n",
    "24. RelationshipSatisfaction (1-4) (Numeric)\n",
    "25. StandardHours (numeric)\n",
    "26. StockOptionLevel (0-3) (Numeric)\n",
    "27. TotalWorkingYears (Numeric)\n",
    "28. TrainingTimesLastYear (0-5) (Numeric)\n",
    "29. WorkLifeBalance (1-4)(Numeric)\n",
    "30. YearsAtCompany (Numeric)\n",
    "31. YearsInCurrentRole (Numeric)\n",
    "32. YearsSinceLastPromotion(Numeric)\n",
    "33. YearsWithCurrManager (Numeric)\n",
    "\n",
    "#### Target (y)\n",
    "- Attrition (Binary)\n",
    "\n",
    "#### Objective\n",
    "- To apply Logistic Regression and Decision Tree Algorithms on the given dataset and understand the concepts of Underfitting and Overfitting and ways to combat these problems. There are three sections -\n",
    "- Section 1 \n",
    "    - Visualization of Overfitting and Underfitting (with results on cross validation)\n",
    "- Section 2\n",
    "    - Regularization\n",
    "\n",
    "\n",
    "#### Tasks\n",
    "- Download and load the data (csv file)\n",
    "- Process the data according to guidelines given in the comments of the respective cells.\n",
    "- Split the dataset into 60% for training and rest 40% for testing (sklearn.model_selection.train_test_split function)\n",
    "- Initialize Logistic Regression and Decision Tree Models (With parameters given in the cell)\n",
    "- Train the models on the same dataset\n",
    "- Complete Section 1 and 2\n",
    "\n",
    "#### Further Fun (will not be evaluated)\n",
    "- Train model on different train-test splits such as 60-40, 50-50, 70-30, 80-20, 90-10, 95-5 etc. and observe the respective plots and results on both X_train and X_test\n",
    "- Shuffle training samples with different random seed values in the train_test_split function. Check the model error for the testing data for each setup.\n",
    "- Explore ways to deal with imbalanced dataset. Use different methods (such as eliminating outliers and such) to experiment with the given dataset.\n",
    "\n",
    "#### Helpful links\n",
    "- pd.get_dummies() and One Hot Encoding: https://queirozf.com/entries/one-hot-encoding-a-feature-on-a-pandas-dataframe-an-example\n",
    "- Feature Scaling: https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "- Train-test splitting: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "- Differences between Logistic Regression and a Decision Tree: https://www.geeksforgeeks.org/ml-logistic-regression-v-s-decision-tree-classification/\n",
    "- When are Decision Trees better than Logistic Regression?: https://www.displayr.com/decision-trees-are-usually-better-than-logistic-regression\n",
    "- How to choose between Logistic Regression and Decision Trees given a dataset: https://datascience.stackexchange.com/questions/6048/should-i-use-a-decision-tree-or-logistic-regression-for-classification\n",
    "- Decision Tree Classifier by Sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "- Regularization and Geometry- https://towardsdatascience.com/regularization-and-geometry-c69a2365de19\n",
    "- RidgeClassifier- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html\n",
    "- Use slack for doubts: https://join.slack.com/t/deepconnectai/shared_invite/zt-givlfnf6-~cn3SQ43k0BGDrG9_YOn4g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier, RidgeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from data.learning_plot import plot_learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, minmax_scale\n",
    "from sklearn import linear_model\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Git clone the repo \n",
    "!git clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from local cloud directory\n",
    "data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dataframe rows just to see some samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print info about dataset\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Missing Values (if any)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encode Categorical Columns (if required)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize/Standardize numerical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data (60/40)\n",
    "X_train, X_test, y_train, y_test = train_test_split(?, ?, test_size=?, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will be introducing you to a practical visualization of the concepts of Overfitting and Underfitting alongside Cross Validation as a metric to measure performance of your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the accuracies we get when the models overfit (LR and DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the models \n",
    "lr=LogisticRegression(class_weight='balanced',penalty='none',fit_intercept=False)\n",
    "dt=DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the models\n",
    "lr.fit(?,?)\n",
    "dt.fit(?,?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the accuracies of the training and test splits for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------LOGISTIC REGRESSION----------\")\n",
    "print(\"Accuracy of Training Split :\",accuracy_score(?,?))\n",
    "print(\"Accuracy of Test Split :\",accuracy_score(?,?))\n",
    "print()\n",
    "print(\"----------DECISION TREE----------\")\n",
    "print(\"Accuracy of Training Split :\",accuracy_score(?,?))\n",
    "print(\"Accuracy of Test Split :\",accuracy_score(?,?))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could see above, even though both models perform very well on the training set, they fail to show the same promise for the testing result. Now let's visualize our findings.\n",
    "\n",
    "<strong>Note</strong> - For cross validation, one can also use KFolds or StratifiedKFolds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are splitting the original X into 10 train/test splits and\n",
    "reinitializing our models and thereby applying them onto the splits for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation splitting for evaluation of Logistic Regression\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then feed in X, y and the splits into our plot_learning_curve function which is user-defined and already imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Estimator\n",
    "estimator = LogisticRegression(class_weight='balanced',penalty='none')\n",
    "\n",
    "#Plot of Learning Curve (over original X and then cross val)\n",
    "plt.style.use(\"seaborn\")\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "title = \"Learning Curves (Logistic Regression)\"\n",
    "plot_learning_curve(estimator, title, ?, ?, axes=axes, ylim=(0.7, 1.01),\n",
    "                    cv=?, n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation splitting for evaluation of Decision Tree\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Estimator\n",
    "estimator = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of Learning Curve (over original X and then cross val)\n",
    "title = r\"Learning Curves (Decision Tree)\"\n",
    "plt.style.use(\"seaborn\")\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "plot_learning_curve(estimator, title, ?, ?, axes=axes, ylim=(0.7, 1.01),\n",
    "                    cv=?, n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the accuracies we get when the models underfit (LR and DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the models\n",
    "lr=LogisticRegression(fit_intercept=False,class_weight='balanced',C=0.001)\n",
    "dt=DecisionTreeClassifier(max_leaf_nodes=2,max_features='log2',class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the models\n",
    "lr.fit(?,?)\n",
    "dt.fit(?,?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the accuracies of the training and test splits for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------LOGISTIC REGRESSION----------\")\n",
    "print(\"Accuracy of Training Split :\",accuracy_score(?,?))\n",
    "print(\"Accuracy of Test Split :\",accuracy_score(?,?))\n",
    "print()\n",
    "print(\"----------DECISION TREE----------\")\n",
    "print(\"Accuracy of Training Split :\",accuracy_score(?,?))\n",
    "print(\"Accuracy of Test Split :\",accuracy_score(?,?))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could see above, both models perform poorly as far as training is concerned. As a result, we also perform poorly on the testing set. This happened because of the hyperparameters set in a specific way. Now, let's visualize this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation splitting for evaluation of Logistic Regression\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Estimator\n",
    "estimator = LogisticRegression(fit_intercept=False,class_weight='balanced',C=0.001)\n",
    "\n",
    "#Plot of Learning Curve (over original X and then cross val)\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "title = \"Learning Curves (Logistic Regression)\"\n",
    "plot_learning_curve(estimator, title, ?, ?, axes=axes, ylim=(0.1, 1.01),\n",
    "                    cv=?, n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation splitting for evaluation of Decision Tree\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Estimator\n",
    "estimator = DecisionTreeClassifier(max_leaf_nodes=2,max_features='log2',class_weight='balanced')\n",
    "\n",
    "#Plot of Learning Curve (over original X and then cross val)\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "title = \"Learning Curves (Decision Tree)\"\n",
    "plot_learning_curve(estimator, title, ?, ?, axes=axes, ylim=(0.1, 1.01),\n",
    "                    cv=?, n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha: Regularization Strength, Larger values specify stronger regularization\n",
    "alphas = np.logspace(10, -3, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Ridge CLassifier on different values of alpha\n",
    "ridge_coefs = []\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "for a in alphas:\n",
    "    ridge = RidgeClassifier(alpha = a, fit_intercept = True, normalize = True)\n",
    "    ridge.fit(?,?)\n",
    "    train_losses.append(log_loss(?, ridge._predict_proba_lr(?)))\n",
    "    test_losses.append(log_loss(?, ridge._predict_proba_lr(?)))\n",
    "    ridge_coefs.append(ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make ridge_coefs numpy array of shape (no_of_alphas,no_of_features)\n",
    "ridge_coefs ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot showing how coefficients vary with value of alpha\n",
    "plt.style.use(\"seaborn\")\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, ridge_coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Value of Lambda')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for Train Loss v/s Values of alpha\n",
    "plt.style.use(\"seaborn\")\n",
    "ax = plt.gca()\n",
    "plt.plot(alphas, train_losses)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Value of Lambda')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for Test Loss v/s Values of alpha\n",
    "plt.style.use(\"seaborn\")\n",
    "ax = plt.gca()\n",
    "plt.plot(alphas, test_losses)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Value of Lambda')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c : Inverse of regularization strength; smaller values specify stronger regularization.\n",
    "C= np.logspace(-10,3,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training logisitic Regression with l1 penalty for different values of C\n",
    "lasso_coefs = []\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "for c in C:\n",
    "    lr = LogisticRegression(penalty='l1', C=c, fit_intercept=True, solver='liblinear')\n",
    "    lr.fit(?,?)\n",
    "    train_losses.append(log_loss(?, lr.predict_proba(?)))\n",
    "    test_losses.append(log_loss(?,lr.predict_proba(?)))\n",
    "    lasso_coefs.append(lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make lasso_coefs numpy array of shape (no_of_C,no_of_features)\n",
    "lasso_coefs="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot showing how coefficients vary with value of c\n",
    "plt.style.use(\"seaborn\")\n",
    "ax = plt.gca()\n",
    "ax.plot(C, lasso_coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Value of C')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for Train Loss v/s Values of C\n",
    "plt.style.use(\"seaborn\")\n",
    "ax = plt.gca()\n",
    "plt.plot(C, train_losses)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Value of C')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for Train Loss v/s Values of C\n",
    "plt.style.use(\"seaborn\")\n",
    "ax = plt.gca()\n",
    "plt.plot(C, test_losses)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Value of C')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
